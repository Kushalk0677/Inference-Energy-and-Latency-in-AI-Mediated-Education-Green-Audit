# Inference Energy and Latency in AI-Mediated Education: A Learning-per-Watt Analysis of Edge and Cloud Models
Abstract

Immediate feedback is a foundational requirement of effective AI-mediated learning, yet the energy and latency costs of delivering it remain largely unexamined in educational research. This study investigates the latency–energy–learning trade-off in AI tutoring systems through an empirical comparison of two on-device inference configurations of Microsoft Phi-3 Mini (4k-instruct) on an NVIDIA T4 GPU: full-precision FP16 and 4-bit NormalFloat (NF4) quantisation. Both configurations were evaluated across 500 educational prompts spanning five secondary school subject domains. Pedagogical quality was assessed empirically for each of the 1,000 generated responses by a hybrid panel of 10 Cambridge International secondary school teachers and three frontier AI systems, using a four-dimension rubric covering conceptual accuracy, clarity, scaffolding quality, and level appropriateness. To formalise the relationship between energy, latency, and instructional quality, we introduce Learning-per-Watt (LpW), a novel metric that quantifies pedagogical value delivered per unit of energy expended over the learner's waiting window. Results reveal that NF4 quantisation does not realise its theoretical efficiency gains on the T4 GPU: mean latency increased from 16.5 s (FP16) to 49.4 s (NF4), and mean net energy per inference rose from 648 J to 1,882 J, yielding an 8.7-fold reduction in LpW despite only a 0.09-point difference in pedagogical quality. These findings demonstrate that quantisation efficiency is hardware-dependent and must be empirically validated rather than assumed, with significant implications for equitable deployment of AI tutoring systems in low-resource educational settings.


