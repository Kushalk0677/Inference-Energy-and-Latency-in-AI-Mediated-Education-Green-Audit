# Inference Energy and Latency in AI-Mediated Education: A Learning-per-Watt Analysis of Edge and Cloud Models
Abstract

As generative artificial intelligence (AI) becomes integral to digital education, reliance on low-latency, high-frequency inference introduces a critical but underexamined constraint: energy availability. Educational research emphasizes immediate feedback as essential for learner autonomy, cognitive flow, and effective scaffolding, yet the inference-time energy cost required to sustain such feedback is rarely evaluated within a pedagogical framework. This study presents a comparative electrical engineering audit of cloud-based Large Language Models (LLMs) and on-device Small Language Models (SLMs), focusing on inference energy, latency, and pedagogical adequacy during realistic educational scaffolding tasks. Using real-time hardware telemetry (CodeCarbon and Intel Power Gadget), we measure power consumption (W), total energy expenditure (J), and response latency across representative instructional prompts. Contrary to common assumptions, results show that naive 4-bit NF4 quantization on commodity edge hardware can increase inference latency and energy consumption relative to FP16 execution, leading to substantially lower pedagogical efficiency. To formalize the trade-off between learning effectiveness and physical constraints, we introduce the Learning-per-Watt (LpW) Index, integrating pedagogical quality, latency, and energy consumption into a single evaluative metric. The findings identify a quantifiable Power Barrier, beyond which inference energy demands make timely pedagogical feedback infeasible on battery-powered or intermittently powered devices. Together, these results demonstrate that energy-aware AI inference is a necessary condition for equitable EdTech deployment and that efficiency claims must be empirically validated within real hardwareâ€“software stacks rather than assumed from model compression alone.
